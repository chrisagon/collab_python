{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création automatisé du fichier Planning de Fiabilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier *Planning de Fiabilisation* est composé de plusieurs onglets :\n",
    "* un onglet __Synthèse__ récapitule les nombres d'occurrences chargées et rejetées.\n",
    "* un onglet __Calendrier__ récapitule par lot (AG01, AG02...) le nombre de rejet avec une courte analyse et un rappel du nombre de rejet du tir précédent.\n",
    "* un onglet __LG11__ récapitule pour chaque occurrence rejeté le code rejet avec son libellé avec les rubriques de la table technique LG11\n",
    "\n",
    "Nous avons mis au point ce script PYTHON parce que nous avions des problème de performance avec les fichiers XLS que nous utilisions auparavent. Les copier/coller avec le RECHERCHEV sur des dizaines de milliers de lignes mettaient notre PC à genoux. Nous proposons avec ce script une méthode plus rapide et intelligente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'analyse des rejets\n",
    "\n",
    "Nous allons avoir en entrée 2 fichiers :\n",
    "\n",
    "* un fichier __base de capitalisation__ qui contient tous les rejets avec les analyses correspondantes à ces rejets,\n",
    "* un fichier __REJET_LG11__ qui contient dossiers rejetés\n",
    "\n",
    "Nous allons avoir en sortie 1 fichier __Planning de Fiabilisation__ avec au moins 2 onglets :\n",
    "\n",
    "* un onglet  __LG11__ récapitule pour chaque occurrence rejeté le code rejet avec son libellé avec les rubriques de la table technique LG11\n",
    "* un onglet __Calendrier__ récapitule par lot (AG01, AG02...) le nombre de rejet avec une courte analyse.\n",
    "\n",
    "Pour faire le lien entre un rejet capitalisé et une ligne rejeté, il faut *assembler* une clé qui est légèrement différente suivant le **Type d'erreur**. Il y a 3 types d'erreurs LG11 :\n",
    "\n",
    "* Compatibilité : détecté par le traitement d'injection NRB (mouvements en double...)\n",
    "* Dictionnaire : détecté par les traitements intrinsèques du Dictionnaire de données HRAccess.\n",
    "* Utilisateur : détecté par un traitement *fonctionnel* (BNA/BNK) avec un code erreur.\n",
    "\n",
    "#### Constitution de la clé/index de recherche des Rejets\n",
    "\n",
    "Suivant le type de rejet, nous allons constituer une clé de recherche discriminante que nous allons implémenter pour la  __base de capitalisation__ et pour les __REJET_LG11__ .\n",
    "\n",
    "S'il s'agit d'un type d'erreur **Compatibilité**, il faut concaténer les rubriques : *INFORMATION_HRA* + *LIB_ERREUR*\n",
    "\n",
    "S'il s'agit d'un type d'erreur **Dictionnaire**, il faut concaténer les rubriques : *INFORMATION_HRA* + *CODE_RUBRIQUE* + *LIB_ERREUR*\n",
    "\n",
    "S'il s'agit d'un type d'erreur **Utilisateur**, il faut concaténer les rubriques : *INFORMATION_HRA* + *CODE_ERREUR*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etablir la connection avec le classeur __base de capitalisation__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "os.environ ['PATH'] \n",
    "__version__ = '1.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import sys\n",
    "from datetime import time, tzinfo, timedelta\n",
    "class GMT1(tzinfo):\n",
    "    def utcoffset(self, dt):\n",
    "        return timedelta(hours=1)\n",
    "    def dst(self, dt):\n",
    "        return timedelta(0)\n",
    "    def tzname(self,dt):\n",
    "        return \"Europe/Paris\"\n",
    "t= dt.datetime.now()\n",
    "print(\"Début du traitement à \",t.strftime(\"%H:%M:%S %Z\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons lire le fichier qui contient notre base de capitalisation. La base de capitalisation est un fichier dans lequel nous avons consigné tous les rejets que nous avons identifiés avec leur cause, ainsi qu'un moyen de corriger. Le fichier doit être dans le même répertoire que ce script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture du classeur __base de capitalisation__\n",
    "t= dt.datetime.now()\n",
    "print(\"Lecture de la base de Capitalisation à \",t.strftime(\"%H:%M:%S %Z\"))\n",
    "wbBdC = xw.Book('anarejetsv1.xlsx')\n",
    "# Conversion du tableau en panda.dataframe\n",
    "feuilleBdC=wbBdC.sheets[0]\n",
    "pdBdC = feuilleBdC.range('A1').options(pd.DataFrame,\n",
    "                         index=False, expand='table').value\n",
    "pdBdC.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous recréons un index identique entre la base de capitalisation et la LG11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour chaque ligne : création de la clé\n",
    "# print(pdBdC.columns) # pour identifier le nom des colonnes\n",
    "t= dt.datetime.now()\n",
    "print(\"Création de l'index des rejets de la base de capitalisation à \",t.strftime(\"%H:%M:%S %Z\"))\n",
    "for label, row in pdBdC.iterrows() :\n",
    "    if row['Type_erreur'] == \"Dictionnaire\" :\n",
    "        Cle_rejet=str(row['INFO']) + str(row ['Rubriques Hra']) + str(row['Libellé ou Code erreur'])[:32]\n",
    "    if row['Type_erreur'] == \"Compatibilité\" or row['Type_erreur'] == \"CompatibilitÃ©\":\n",
    "        Cle_rejet=str(row[\"INFO\"]) + str(row['Libellé ou Code erreur'])\n",
    "    if row['Type_erreur'] == \"Utilisateur\" :\n",
    "        Cle_rejet=str(row[\"INFO\"]) + str(row['Libellé ou Code erreur'])[:8]\n",
    "    pdBdC.loc[label,\"Clé Rejet\"]=Cle_rejet\n",
    "    \n",
    "# Redéfinir le nouvel index\n",
    "pdBdC.reset_index(inplace=True)\n",
    "pdBdC.set_index(['Clé Rejet'], inplace=True)\n",
    "\n",
    "# Alimentation du tableau de recherche __base de capitalisation__\n",
    "# création d'un dataframe avec \"Clé Rejet\" et \"Analyse CISIRH\" uniquement pour fusion\n",
    "# pdBdC_analyse=pd.DataFrame(pdBdC['Analyse CISIRH '].describe().tolist(), columns =['Analyse'])\n",
    "pdBdC_analyse=pd.DataFrame({'ID Rejet': pdBdC['ID'], 'Analyse': pdBdC['Analyse CISIRH ']})\n",
    "pdBdC_analyse.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cette étape, nous allons lire l'extraction LG11 au format CSV. Le fichier doit être dans le même répertoire que ce script.\n",
    "### Indiquez ici le nom du fichier LG11 à analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du classeur __REJET_LG11__\n",
    "wbLG11 = xw.Book('REJET_AGENT_LOT1_MAA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence à partir de la ligne d'entête qui commence avec \"TYPE_DOSSIER\"\n",
    "feuilleLG11=wbLG11.sheets[0]\n",
    "for i,cellule in enumerate(feuilleLG11.range('A1:A95')):\n",
    "    if cellule.value == \"TYPE_DOSSIER\" :\n",
    "        debutLG11 = cellule.address\n",
    "        print (\"Début de la LG11 :\", debutLG11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du tableau en panda.dataframe\n",
    "feuilleLG11=wbLG11.sheets[0]\n",
    "pdLG11 = feuilleLG11.range(debutLG11).options(pd.DataFrame,\n",
    "                         index=False, expand='table').value\n",
    "# pdLG11.dropna() # pour supprimer les lignes vides\n",
    "\n",
    "# print(pdLG11.columns) # pour debug afficher les colonnes du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t= dt.datetime.now()\n",
    "print (\"Création de l'index des rejets de la LG11 à \",t.strftime(\"%H:%M:%S %Z\"))\n",
    "# Pour chaque ligne : création de la clé\n",
    "for label, row in pdLG11.iterrows() :\n",
    "    if row['GRAVITE'] == \"Erreur bloquante\":\n",
    "        if row['TYPE_ERREUR'] == \"Dictionnaire\" :\n",
    "            Cle_rejet=str(row['INFORMATION_HRA']) + str(row ['CODE_RUBRIQUE']) + str(row['LIB_ERREUR'])[:32]\n",
    "        if row['TYPE_ERREUR'] == \"Compatibilité\" or row['TYPE_ERREUR'] == \"CompatibilitÃ©\" :\n",
    "            Cle_rejet=str(row['INFORMATION_HRA']) + str(row['LIB_ERREUR'])\n",
    "        if row['TYPE_ERREUR'] == \"Utilisateur\" :\n",
    "            Cle_rejet=str(row['INFORMATION_HRA']) + str(row['CODE_ERREUR'])[:8]\n",
    "        pdLG11.loc[label,\"Clé Rejet\"]=Cle_rejet\n",
    "  \n",
    "# Redéfinir le nouvel index\n",
    "pdLG11.reset_index(inplace=True)\n",
    "try: \n",
    "    pdLG11.set_index(['Clé Rejet'], inplace=True)\n",
    "except KeyError:\n",
    "    print(\"Pas de rejets trouvés !\")\n",
    "# pdLG11.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les rejets non documentés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ATTENTION__ : Il faut vérifier que toutes les clés de rejets de la LG11 existe dans la Base de Capitalisation.\n",
    "On créé un dataframe temporaire (pdLG11_analyse) pour stocker les codes rejets et détecter plus facilement les codes rejets absents de la base de capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdLG11_analyse=pd.DataFrame(pdLG11.iloc[:,3])\n",
    "# print(pdLG11_analyse) # pour debug\n",
    "\n",
    "# le tilde permet de NE PAS selectionner toutes les clés de rejet de pdLG11 qui sont dans pdBdC\n",
    "Rejets_non_trouves = pdLG11_analyse[~pdLG11_analyse.index.isin(pdBdC_analyse.index)]\n",
    "# print(Rejets_non_trouves) # pour debug\n",
    "# On supprime les doublons\n",
    "Rejets_uniques_NA = Rejets_non_trouves.index.drop_duplicates().dropna()\n",
    "# print(Rejets_uniques_NA) # pour debug\n",
    "# !!! LISTE DES REJETS A AMELIORER !!!\n",
    "liste_rejets_NA=list(Rejets_uniques_NA)\n",
    "# Si la liste contient au moins 1 rejet : on liste\n",
    "for rejet in liste_rejets_NA :\n",
    "    print(\"Code non trouvé \",rejet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion de la table de capitalisation avec la table LG11 pour obtenir le *planning de fiabilisation* ###\n",
    "\n",
    "Après avoir stocké dans des tables Panda nos données, nous allons fusionner ces 2 tables sur la Clé Rejet avec *pd.merge*.\n",
    "Nous allons créé un fichier Excel vide dans lequel nous allons mettre le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t= dt.datetime.now()\n",
    "print(\"Création du Planning de fiabilisation à \",t.strftime(\"%H:%M:%S %Z\"))\n",
    "# recherche dans tableau de recherche __base de capitalisation__\n",
    "# avec le mécanisme de fusion Panda (pd.merge) du tableau LG11 et du tableau Base de connaissances\n",
    "# Voir pour la création d'un nom de Classeur à partir du Lot LG11\n",
    "print (\"Création du classeur \")\n",
    "wbPdF = xw.Book()  # Ceci va créer un nouveau classeur Planning de Fiabilisation\n",
    "# création de la feuille LG11\n",
    "feuillePdF = wbPdF.sheets[0]\n",
    "\n",
    "# Alimentation de l'entête du fichier LG11\n",
    "feuillePdF.range('A1').value = ['ID','Analyse_CISIRH','TYPE_DOSSIER','IDENTIFIANT_DOSSIER_HRA','ANCIEN_IDENTIFIANT','ID1_OCCURRENCE','ID2_OCCURRENCE','ID3_OCCURRENCE','ID4_OCCURRENCE','ID5_OCCURRENCE','ID6_OCCURRENCE','DATE_REJET','HORODATAGE_ERREUR','INFORMATION_HRA','TNAME','CODE_GRAVITE','GRAVITE','CODE_RUBRIQUE','CNAME','VALEUR','TYPE_ERREUR','LIB_ERREUR','CODE_ERREUR','CODE_TRAITEMENT','REPERTOIRE_HRA','NOMENCLATURE','ID_TIR','zontri'\n",
    "]\n",
    "# Cela nous permet de récupérer l'instance spécifique d'Excel avec son pid\n",
    "pid = xw.apps.keys()[0] # ou vous pouvez utiliser xw.apps.active.pid\n",
    "try:\n",
    "    planning_fiab1=pd.merge(pdLG11, pdBdC_analyse,on='Clé Rejet',how='left')\n",
    "except KeyError:\n",
    "    print(\"Pas de tableau à générer !\")\n",
    "\n",
    "# Ecriture dans le fichier Planning de Fiabilisation Onglet LG11\n",
    "\n",
    "feuillePdF.range('A1').options(index=False).value = pd.DataFrame(planning_fiab1, columns=['ID Rejet', 'Analyse','TYPE_DOSSIER', 'IDENTIFIANT_DOSSIER_HRA',\n",
    "       'ANCIEN_IDENTIFIANT', 'ID1_OCCURRENCE', 'ID2_OCCURRENCE',\n",
    "       'ID3_OCCURRENCE', 'ID4_OCCURRENCE', 'ID5_OCCURRENCE', 'ID6_OCCURRENCE',\n",
    "       'DATE_REJET', 'HORODATAGE_ERREUR', 'INFORMATION_HRA', 'TNAME',\n",
    "       'CODE_GRAVITE', 'GRAVITE', 'CODE_RUBRIQUE', 'CNAME', 'VALEUR',\n",
    "       'TYPE_ERREUR', 'LIB_ERREUR', 'CODE_ERREUR', 'CODE_TRAITEMENT',\n",
    "       'REPERTOIRE_HRA', 'NOMENCLATURE', 'ID_TIR', 'zontri'])\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A la fin de la lecture du classeur __REJET_LG11__ : création de l'onglet __Calendrier__\n",
    "# Création de la feuille de statistique des rejets\n",
    "feuilleStatPdF = wbPdF.sheets.add(name='Stats', after='Feuil1')\n",
    "# \n",
    "t= dt.datetime.now()\n",
    "print(\"Création du Compte-rendu des rejets à \",t.strftime(\"%H:%M:%S %Z\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage du nombre de rejets en groupant par ID Rejet & Analyse\n",
    "statrejets = planning_fiab1.groupby(['ID Rejet', 'Analyse'], as_index=False).count()\n",
    "# Ecriture dans l'onglet Stats de 3 colonnes uniquement 'ID Rejet', 'Analyse', 'INFORMATION_HRA'\n",
    "feuilleStatPdF.range('A1').options(index=False).value = pd.DataFrame(statrejets,  columns=['ID Rejet', 'Analyse', 'INFORMATION_HRA'])\n",
    "\n",
    "# Petite mise en page avec\n",
    "# autofit des colonnes à partir d'une plage\n",
    "feuilleStatPdF.range('A:C').columns.autofit()\n",
    "# Assigne un tuple RGB Gris\n",
    "feuilleStatPdF.range('A1:C1').color = (216, 216, 216)\n",
    "#\n",
    "t= dt.datetime.now()\n",
    "print(\"Terminé à \",t.strftime(\"%H:%M:%S %Z\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à Copier/Coller le résultat dans le fichier consolidé."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
